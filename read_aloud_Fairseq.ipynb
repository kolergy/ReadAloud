{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "conda env remove -n spkF\n",
    "conda create --name spkF python=3.9  ipywidgets ipykernel -y\n",
    "conda activate spkF\n",
    "pip install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "pip install -U fairseq phonemizer sentencepiece g2p_en\n",
    "pip install -U huggingface_hub\n",
    "pip install -U pyttsx3\n",
    "pip install -U bs4 nltk scipy\n",
    "pip install -U langdetect playsound\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "sudo apt install espeak\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "import requests\n",
    "import pyttsx3\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "import wave  \n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "from langdetect       import detect\n",
    "from bs4              import BeautifulSoup, __version__\n",
    "from scipy.io.wavfile import write\n",
    "from nltk.tokenize    import sent_tokenize\n",
    "from playsound        import playsound\n",
    "\n",
    "from fairseq.checkpoint_utils                    import load_model_ensemble_and_task_from_hf_hub\n",
    "from fairseq.models.text_to_speech.hub_interface import TTSHubInterface\n",
    "\n",
    "\n",
    "print(f\"requests     : {requests.__version__ }\")\n",
    "print(f\"torch        : {    torch.__version__}\")\n",
    "print(f\"BeautifulSoup: {          __version__}\")\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "langs  = {v.languages[0].decode()[1:]: v.id for v in engine.getProperty('voices')}\n",
    "engine.setProperty('rate', 200)\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "models_fr, cfg_fr, task_fr = load_model_ensemble_and_task_from_hf_hub(\n",
    "    \"facebook/tts_transformer-fr-cv7_css10\",\n",
    "    arg_overrides={\"vocoder\": \"hifigan\", \"fp16\": False, \"cpu\":True}\n",
    ")\n",
    "model_fr     = models_fr[0]\n",
    "TTSHubInterface.update_cfg_with_data_cfg(cfg_fr, task_fr.data_cfg)\n",
    "generator_fr = task_fr.build_generator(models_fr, cfg_fr)\n",
    "\n",
    "\n",
    "models_en, cfg_en, task_en = load_model_ensemble_and_task_from_hf_hub(\n",
    "    \"facebook/fastspeech2-en-ljspeech\",\n",
    "    arg_overrides={\"vocoder\": \"hifigan\", \"fp16\": False, \"cpu\":True}\n",
    ")\n",
    "model_en     = models_en[0]\n",
    "TTSHubInterface.update_cfg_with_data_cfg(cfg_en, task_en.data_cfg)\n",
    "generator_en = task_en.build_generator(models_en, cfg_en)\n",
    "\n",
    "out_file = \"tts_out.wav\"\n",
    "sr       = 22050\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sentences(text):\n",
    "    \n",
    "    sent_raw   = sent_tokenize(text.replace('\\t', ' ')) # Tokenize the string into sentences\n",
    "    #print(sent_raw)\n",
    "    sentences  = []\n",
    "    #lemizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "    sentences = [\" \".join([word for word in s.split() ]) for s in sent_raw ]\n",
    "    sentences = list(map(lambda s: s.rstrip(), sentences))\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text   = \"This is a test to check, if a simple text can be spoken\"\n",
    "sentenses  = filter_sentences(raw_text)\n",
    "print(sentenses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"https://fr.wikipedia.org/wiki/Saison_cyclonique_2022-2023_dans_l%27oc%C3%A9an_Indien_sud-ouest\"\n",
    "#url = \"https://en.wikipedia.org/wiki/Fab_lab\"\n",
    "#url = \"https://artilect.fr\"\n",
    "url      = None\n",
    "txt_file = None\n",
    "#txt_file = \"file.txt\"\n",
    "#sentenses = []\n",
    "if url != None:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception raised: {e}\")\n",
    "        sentenses = []\n",
    "\n",
    "    if response.status_code==200: \n",
    "        print(\"Successfully opened the web page\")\n",
    "\n",
    "        soup       = BeautifulSoup(response.text,'html.parser')\n",
    "        lang       = soup.find('lang')\n",
    "        heading    = soup.select(\"#firstHeading\")\n",
    "        #print(heading)\n",
    "        if len(heading) < 1:\n",
    "            title    = soup.find(\"title\").get_text()\n",
    "        else:\n",
    "            title    = heading[0].text\n",
    "        #sentenses  = [s for para in paragraphs for s in sent_tokenize(para.text)]\n",
    "        raw_text   = \"\"\n",
    "        paragraphs = soup.select(\"p\")\n",
    "        for p in paragraphs: raw_text   += p.text\n",
    "        sentenses  = filter_sentences(raw_text)\n",
    "\n",
    "    else: \n",
    "        print(f\"Error: {response.status_code}\")                      \n",
    "        sentenses = []\n",
    "\n",
    "\n",
    "elif txt_file is not None:\n",
    "    raw_text   = open(txt_file).read()\n",
    "    sentenses  = filter_sentences(raw_text)\n",
    "#elif sentenses is not none:\n",
    "\n",
    "#print(langs)\n",
    "if len(sentenses) > 0:\n",
    "  \n",
    "    print(sentenses)\n",
    "    n = 0\n",
    "    for s in sentenses:\n",
    "        file        = str(n) + '_' + out_file\n",
    "        try:\n",
    "            lang = detect(s)  #only first line \n",
    "            #lang = 'fr'\n",
    "            print(lang)\n",
    "        except Exception as e:\n",
    "            if lang==None: lang = 'en'\n",
    "        \n",
    "        print(f\"[{lang}]: {s}\")\n",
    "        if lang=='en':\n",
    "\n",
    "            sample_en = TTSHubInterface.get_model_input(task_en, s)\n",
    "            audio, sr = TTSHubInterface.get_prediction(task_en, model_en, generator_en, sample_en)\n",
    "            write(file, sr, audio.to('cpu').detach().numpy())\n",
    "            playsound(file)\n",
    "            #ipd.Audio(file)\n",
    "        elif lang=='fr':\n",
    "            sample    = TTSHubInterface.get_model_input(task_fr, s)\n",
    "            audio, sr = TTSHubInterface.get_prediction(task_fr, model_fr, generator_fr, sample)\n",
    "            write(file, sr, audio.to('cpu').detach().numpy())\n",
    "            playsound(file)\n",
    "        else:\n",
    "            if not lang in (langs.keys() or ['en', 'fr']):\n",
    "                lang = lang + '-' + lang\n",
    "                if not lang in langs.keys():\n",
    "                    print(f\"WARNING! language {lang} not supported defaulting to english\")\n",
    "                    lang = 'en'\n",
    "            engine.setProperty(\"voice\", langs[lang])\n",
    "            engine.say(s)\n",
    "            engine.runAndWait()\n",
    "        \n",
    "        n += 1\n",
    "\n",
    "else: print(f\"Error: No text to say\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spkF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 15:55:03) \n[GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a7abff741de943d98d86c4c8996c5934eef837032e48a75dc800ac0b9bdecd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
